"""Linear Score-based Causal Latent Estimation via Interventions (LSCALE-I)

TODO: Add some file-wide comments.
"""

__all__ = ["l_scale_i"]

import logging

import numpy as np
import numpy.typing as npt

import utils


def l_scale_i(
    x_samples: npt.NDArray[np.float_],
    dsx_samples: npt.NDArray[np.float_],
    hard_intervention: bool,
    full_rank_scores: bool,
    hard_graph_postprocess: bool,
    atol_eigv: float,
    atol_orth: float,
    atol_edge: float,
) -> tuple[
    npt.NDArray[np.int_],
    npt.NDArray[np.bool_],
    npt.NDArray[np.float_],
    tuple[
        npt.NDArray[np.bool_],
        npt.NDArray[np.float_],
    ]
    | None,
]:
    assert dsx_samples.ndim == 4 and dsx_samples.shape[3] == 1
    n, nsamples, d, _ = dsx_samples.shape
    assert x_samples.shape == (n + 1, nsamples, d, 1)

    # Preprocessing:
    # x and relevant parts of dsx lives in `n` dimensional column space of the decoder.
    # Find this subspace and write x and dsx samples in this basis.
    # Note that `n` random samples almost surely suffice for this task. I use `n + d`
    # samples just in case.
    x_cov = utils.cov(x_samples[:, : n + d, :, 0])
    _, dec_svec = np.linalg.eigh(np.sum(x_cov, 0))
    dec_colbt = dec_svec[:, -n:].T
    x_n_samples = dec_colbt @ x_samples
    dsx_n_samples = dec_colbt @ dsx_samples

    # We can express the algorithm entirely in terms of covariance & correlation matrices
    x_n_cov = utils.cov(x_n_samples[..., 0])
    dsx_n_cor = utils.cov(dsx_n_samples[..., 0], center_data=False)

    # Run algorithm steps
    top_order = _obtain_causal_order(dsx_n_cor)
    hat_g_s, hat_enc_n_s = _minimize_score_variations(
        dsx_n_cor,
        top_order,
        full_rank_scores,
        atol_eigv,
        atol_orth,
    )

    # Transform the encoder estimates back up to `d` dimensions
    hat_enc_s = hat_enc_n_s @ dec_colbt

    # Optional hard intervention routine
    hard_results = None
    if hard_intervention:
        hat_g_h, hat_enc_n_h = _unmixing_procedure(
            x_n_cov,
            dsx_n_cor,
            top_order,
            hat_enc_n_s,
            atol_edge,
        )

        # Transform the encoder estimates back up to `d` dimensions
        hat_enc_h = hat_enc_n_h @ dec_colbt
        hard_results = (hat_g_h, hat_enc_h)

        if hard_graph_postprocess:
            hat_g_h *= hat_g_s

    return top_order, hat_g_s, hat_enc_s, hard_results


def _obtain_causal_order(
    dsx_cor: npt.NDArray[np.float_],
) -> npt.NDArray[np.int_]:
    n, _, _ = dsx_cor.shape
    logging.info(f"Starting `_obtain_causal_order`.")

    vt = list(range(n))
    top_order = np.arange(n)

    # When we find a `k` that increases the kernel dimension, record and continue
    for t in range(n - 1, 0, -1):
        logging.debug(f"{t = }, {vt = }")
        min_eigval_l = np.ones(n) * np.finfo(np.float_).max
        for k in vt:
            vt_new = list[int](i for i in vt if i != k)
            min_eigval_l[k] = np.linalg.eigvalsh(np.sum(dsx_cor[vt_new], 0))[
                n - t - 1
            ]
        youngest_node = np.argmin(min_eigval_l)
        top_order[t] = youngest_node
        vt = list[int](i for i in vt if i != youngest_node)

    # The remaining entry is the oldest
    top_order[0] = vt[0]
    return top_order


def _minimize_score_variations(
    dsx_cor: npt.NDArray[np.float_],
    top_order: npt.NDArray[np.int_],
    full_rank_scores: bool,
    atol_eigv: float,
    atol_orth: float,
) -> tuple[npt.NDArray[np.bool_], npt.NDArray[np.float_]]:
    n, d, _ = dsx_cor.shape
    logging.info(f"Starting `_minimize_score_variations`.")
    logging.debug(f"{atol_eigv = }, {atol_orth = }")

    # We disregard any possible "trivial" null space components:
    # If the original decoder has a null space, _all_ dsx_cor null spaces include it.
    assert n == d

    # We will refine this estimate
    hat_enc_s_pinv_t = np.zeros((n, d))

    # This DAG can correspond to two different things:
    # 1. An estimate of the transitive closure---when the score differences are not,
    #    a priori, known to be full rank, or
    # 2. An estimate of the graph itself.
    # In either case, it is used to exclude environments from the minimization objective.
    hat_g_s = np.zeros((n, n), dtype=np.bool_)

    # V_t sets generated by `_obtain_causal_order`
    vts = [top_order[: t + 1] for t in range(n)]

    for t in range(n - 1, -1, -1):
        logging.debug(f"{t = }, {vts[t] = }")

        # Find vt's column space
        cm = np.sum(dsx_cor[vts[t]], 0)
        cm_eig = np.linalg.eigh(cm)
        cm_eigval: npt.NDArray[np.float_] = cm_eig.eigenvalues
        cm_eigvec: npt.NDArray[np.float_] = cm_eig.eigenvectors
        cm_rank = np.sum(cm_eigval > atol_eigv, dtype=np.int_)
        logging.debug(f"Rank of vt column space: {cm_rank}")
        vt_colb = cm_eigvec[:, -(t + 1) :]

        for j in range(t, n):
            mtj = [
                m for m in vts[j] if m != top_order[t] and not hat_g_s[top_order[t], m]
            ]

            # Find the basis of/orthonormal projector onto mtj
            if len(mtj) == 0:
                # trivial column space => full null space
                logging.debug(f"{j = }, rank of mtj: 0")
                mtj_nullp = np.eye(d)
            else:
                cm = np.sum(dsx_cor[mtj], 0)
                cm_eig = np.linalg.eigh(cm)
                cm_eigval: npt.NDArray[np.float_] = cm_eig.eigenvalues
                cm_eigvec: npt.NDArray[np.float_] = cm_eig.eigenvectors
                cm_rank = np.sum(cm_eigval > atol_eigv, dtype=np.int_)

                # We know a priori that M_{t,t} = V_{t-1} with rank t-1.
                if j == t:
                    cm_rank = t

                logging.debug(f"{j = }, rank of mtj: {cm_rank}")
                mtj_nullb = cm_eigvec[:, :-cm_rank]
                mtj_nullp = mtj_nullb @ mtj_nullb.T

            # We check if there is any vector in null space of mtj but not in vt:
            vtn_on_mtjn = mtj_nullp @ vt_colb
            col_norms: npt.NDArray[np.float_] = np.linalg.norm(
                vtn_on_mtjn, ord=2, axis=0
            )
            if col_norms.size != 0:
                col_norms_argmax = np.argmax(col_norms)
                if col_norms[col_norms_argmax] > atol_orth:
                    # null of mtj **is not** a subspace of null of vt
                    # implied by I^{pi_t} not in pa_{tc}(I^{pi_j}
                    # Update $pi_t$-th row of encoder estimate.
                    hat_enc_s_pinv_t[top_order[t], :] = (
                        vtn_on_mtjn[:, col_norms_argmax] / col_norms[col_norms_argmax]
                    )
                    continue

            # else:
            # null of mtj **is** a subspace of null of vt
            # implies $I^{\pi_{t}} \in \pa_{\mcG_{tr}}(I^{\pi_{j}}$
            # Then there is a transitive reduction edge. Add it.
            hat_g_s[top_order[t], top_order[j]] = True
            if not full_rank_scores:
                # If we are working with transitive closure, update it
                hat_g_s[top_order[t], :] |= hat_g_s[top_order[j], :]

    hat_enc_s = np.linalg.pinv(hat_enc_s_pinv_t.T)

    # Normalize rows of the encoder estimate
    hat_enc_s /= np.linalg.norm(hat_enc_s, ord=2, axis=1, keepdims=True)

    return hat_g_s, hat_enc_s


def _unmixing_procedure(
    x_cov: npt.NDArray[np.float_],
    dsx_cor: npt.NDArray[np.float_],
    top_order: npt.NDArray[np.int_],
    hat_enc_s: npt.NDArray[np.float_],
    atol_edge: float,
) -> tuple[npt.NDArray[np.bool_], npt.NDArray[np.float_]]:
    logging.info(f"Starting `_unmixing_procedure`.")
    logging.debug(f"{atol_edge = }")

    n, _, _ = dsx_cor.shape

    # First, build an unmixing matrix.
    hat_z_cov = hat_enc_s @ x_cov @ hat_enc_s.T
    unmix_mat = np.eye(n)

    for k in range(n):
        hat_z_tilde_z_cov = unmix_mat @ hat_z_cov
        a_mat = hat_z_tilde_z_cov[1 + top_order[k]][
            np.ix_(top_order[:k], top_order[:k])
        ]
        b_vec = hat_z_tilde_z_cov[1 + top_order[k]][top_order[:k], top_order[k]]
        unmix_mat[top_order[k], top_order[:k]] = np.linalg.solve(a_mat, -b_vec)

    hat_enc_h = unmix_mat @ hat_enc_s

    # Normalize rows of the encoder estimate:
    hat_enc_h: npt.NDArray[np.float_] = hat_enc_h / np.linalg.norm(
        hat_enc_h, ord=2, axis=1, keepdims=True
    )

    # Next, estimate the latent graph using the new hat Z score changes.
    hat_g_h = np.zeros((n, n), dtype=np.bool_)
    hat_enc_h_pinvt = np.linalg.pinv(hat_enc_h).T
    dshatz_cor = hat_enc_h_pinvt @ dsx_cor @ hat_enc_h_pinvt.T
    for i in range(n):
        # i-th entry of the score difference is zero iff it has 0 correlation.
        hat_g_h[:, i] = np.diagonal(dshatz_cor[i]) >= atol_edge

    # Remove the i == j case -- these are not actual edges
    hat_g_h &= ~np.eye(n, dtype=np.bool_)

    return hat_g_h, hat_enc_h
